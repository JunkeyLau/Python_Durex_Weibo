{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Python 3 uses utf-8, so you have no need to use reload(sys)\n",
    "#I have two questions, the first one is, could I caculate the page numbers \n",
    "#The second one is, could i download all pictures from the weibo\n",
    "user_id = 1942473263\n",
    "cookie = {\"Cookie\": \"你的cookie\"}\n",
    "url = 'http://weibo.cn/u/%d?filter=1&page=1'%user_id\n",
    "html = requests.get(url, cookies = cookie).content\n",
    "\n",
    "#XML is an inherently hierarchical data format, and the most natural way to represent it is with a tree\n",
    "selector = etree.HTML(html)\n",
    "pageNum = (int)(selector.xpath('//input[@name=\"mp\"]')[0].attrib['value'])\n",
    "\n",
    "result = \"\"\n",
    "urllist_set = set()\n",
    "word_count = 1\n",
    "image_count = 1\n",
    "\n",
    "print(u'ready')\n",
    "print(pageNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#刷新输出\n",
    "sys.stdout.flush()\n",
    "\n",
    "times = 5\n",
    "one_step = pageNum/times\n",
    "\n",
    "for step in range(times):\n",
    "    if step < times - 1:\n",
    "        i = step * one_step + 1\n",
    "        j = (step + 1) * one_step + 1\n",
    "    else:\n",
    "        i = step * one_step + 1\n",
    "        j = pageNum + 1 \n",
    "    for page in range(int(i), int(j)):\n",
    "        try:\n",
    "            url = 'http://weibo.cn/u/%d?filter=1&page=%d'%(user_id, page)\n",
    "            lxml = requests.get(url, cookies = cookie).content\n",
    "            \n",
    "            selector = etree.HTML(lxml)\n",
    "            content = selector.xpath('//span[@class = \"ctt\"]')\n",
    "            for each in content:\n",
    "                text = each.xpath('string(.)')\n",
    "                if word_count >= 3:\n",
    "                    text = \"%d: \"%(word_count - 2) + text + \"\\n\"\n",
    "                else:\n",
    "                    text = text + \"\\n\\n\"\n",
    "                result = result  + text\n",
    "                word_count += 1\n",
    "            print(page, 'word ok')\n",
    "            sys.stdout.flush()\n",
    "            soup = BeautifulSoup(lxml, \"lxml\")\n",
    "            urllist = soup.find_all('a', href=re.compile(r'^http://weibo.cn/mblog/oripic', re.I))\n",
    "            urllist1 = soup.find_all('a', href=re.compile(r'^http://weibo.cn/mblog/picAll', re.I))\n",
    "            for imgurl in urllist:\n",
    "                imgurl['href'] = re.sub(r\"amp;\", '', imgurl['href'])\n",
    "                urllist_set.add(requests.get(imgurl['href'], cookies = cookie).url)\n",
    "                image_count += 1\n",
    "            for imgurl_all in urllist1:\n",
    "                html_content = requests.get(imgurl_all['href'], cookies = cookie).content\n",
    "                soup = BeautifulSoup(html_content, \"lxml\")\n",
    "                urllist2 = soup.find_all('a', href = re.compile(r'^/mblog/oripic', re.I))\n",
    "                for imgurl in urllist2:\n",
    "                    imgurl['href'] = 'http://weibo.cn' + re.sub(r\"amp;\", '', imgurl['href'])\n",
    "                    urllist_set.add(requests.get(imgurl['href'], cookies = cookie).url)\n",
    "                    image_count += 1\n",
    "                image_count -= 1\n",
    "            print(page, 'picurl ok')\n",
    "        except:\n",
    "            print(page, 'error')\n",
    "        print(page, 'sleep')\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(60)\n",
    "    print(u'正在进行第', step + 1, u'次停顿，防止访问次数过多')\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    fo = open(os.getcwd() + \"/%d\"%user_id, \"wb\")\n",
    "    fo.write(result.encode())\n",
    "    word_path = os.getcwd() + '/%d'%user_id \n",
    "    print(u'文字微博爬取完畢')\n",
    "    link = \"\"\n",
    "    fo2 = open(os.getcwd() + \"/%s_image\"%user_id, \"wb\")\n",
    "    for eachlink in urllist_set:\n",
    "        link = link + eachlink + \"\\n\"\n",
    "        fo2.write(link.encode())\n",
    "    print(u'圖片鏈接爬取完畢')\n",
    "except:\n",
    "    print(u'存放數據地址有誤')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for h in urllist_set:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not urllist_set:\n",
    "    print('该用户原创微博中不存在图片')\n",
    "else:\n",
    "    image_path = os.getcwd() + '/weibo_image'\n",
    "    if os.path.exists(image_path) is False:\n",
    "        os.mkdir(image_path)\n",
    "    x = 1\n",
    "    for imgurl in urllist_set:\n",
    "        temp = image_path + '/%s.jpg' % x\n",
    "        print('正在下载第%s张图片'%x)\n",
    "        try:\n",
    "            r = requests.get(imgurl, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(temp, 'wb') as f:\n",
    "                    r.raw.decode_content = True\n",
    "                    shutil.copyfileobj(r.raw, f)\n",
    "        except:\n",
    "            print('该图片下载失败:%s'%imgurl)\n",
    "        x += 1\n",
    "print('原创微博爬取完毕，共%d条，保存路径%s'%(word_count - 3, word_path))\n",
    "print('微博图片爬取完毕，共%d张，保存路径%s'%(image_count - 1, image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
