{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "#Python 3 uses utf-8, so you have no need to use reload(sys)\n",
    "#I have two questions, the first one is, could I caculate the page numbers \n",
    "#The second one is, could i download all pictures from the weibo\n",
    "user_id = 1942473263\n",
    "cookie = {\"Cookie\": \"你的微博Cookie"}\n",
    "url = 'http://weibo.cn/u/%d?filter=1&page=1'%user_id\n",
    "html = requests.get(url, cookies = cookie).content\n",
    "\n",
    "#XML is an inherently hierarchical data format, and the most natural way to represent it is with a tree\n",
    "selector = etree.HTML(html)\n",
    "pageNum = (int)(selector.xpath('//input[@name=\"mp\"]')[0].attrib['value'])\n",
    "\n",
    "result = \"\"\n",
    "urllist_set = set()\n",
    "word_count = 1\n",
    "image_count = 1\n",
    "\n",
    "print(u'ready')\n",
    "print(pageNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 work ok\n",
      "1 picurl ok\n",
      "1 sleep\n",
      "2 work ok\n",
      "2 picurl ok\n",
      "2 sleep\n",
      "3 work ok\n",
      "3 picurl ok\n",
      "3 sleep\n",
      "4 work ok\n",
      "4 picurl ok\n",
      "4 sleep\n",
      "5 work ok\n",
      "5 picurl ok\n",
      "5 sleep\n",
      "6 work ok\n",
      "6 picurl ok\n",
      "6 sleep\n",
      "7 work ok\n",
      "7 picurl ok\n",
      "7 sleep\n",
      "8 work ok\n",
      "8 picurl ok\n",
      "8 sleep\n",
      "9 work ok\n",
      "9 picurl ok\n",
      "9 sleep\n",
      "10 work ok\n",
      "10 picurl ok\n",
      "10 sleep\n",
      "11 work ok\n",
      "11 picurl ok\n",
      "11 sleep\n",
      "12 work ok\n",
      "12 picurl ok\n",
      "12 sleep\n",
      "13 work ok\n",
      "13 picurl ok\n",
      "13 sleep\n",
      "14 work ok\n",
      "14 picurl ok\n",
      "14 sleep\n",
      "15 work ok\n",
      "15 picurl ok\n",
      "15 sleep\n",
      "16 work ok\n",
      "16 picurl ok\n",
      "16 sleep\n",
      "17 work ok\n",
      "17 picurl ok\n",
      "17 sleep\n",
      "18 work ok\n",
      "18 picurl ok\n",
      "18 sleep\n",
      "19 work ok\n",
      "19 picurl ok\n",
      "19 sleep\n",
      "20 work ok\n",
      "20 picurl ok\n",
      "20 sleep\n",
      "21 work ok\n",
      "21 picurl ok\n",
      "21 sleep\n",
      "22 work ok\n",
      "22 picurl ok\n",
      "22 sleep\n",
      "23 work ok\n",
      "23 picurl ok\n",
      "23 sleep\n",
      "24 work ok\n",
      "24 picurl ok\n",
      "24 sleep\n",
      "25 work ok\n",
      "25 picurl ok\n",
      "25 sleep\n",
      "26 work ok\n",
      "26 picurl ok\n",
      "26 sleep\n",
      "27 work ok\n",
      "27 picurl ok\n",
      "27 sleep\n",
      "28 work ok\n",
      "28 picurl ok\n",
      "28 sleep\n",
      "29 work ok\n",
      "29 picurl ok\n",
      "29 sleep\n",
      "30 work ok\n",
      "30 picurl ok\n",
      "30 sleep\n",
      "31 work ok\n",
      "31 picurl ok\n",
      "31 sleep\n",
      "32 work ok\n",
      "32 picurl ok\n",
      "32 sleep\n",
      "33 work ok\n",
      "33 picurl ok\n",
      "33 sleep\n",
      "34 work ok\n",
      "34 picurl ok\n",
      "34 sleep\n",
      "正在进行第 1 次停顿，防止访问次数过多\n",
     ]
    }
   ],
   "source": [
    "#刷新输出\n",
    "sys.stdout.flush()\n",
    "\n",
    "times = 5\n",
    "one_step = pageNum/times\n",
    "\n",
    "for step in range(times):\n",
    "    if step < times - 1:\n",
    "        i = step * one_step + 1\n",
    "        j = (step + 1) * one_step + 1\n",
    "    else:\n",
    "        i = step * one_step + 1\n",
    "        j = pageNum + 1 \n",
    "    for page in range(int(i), int(j)):\n",
    "        try:\n",
    "            url = 'http://weibo.cn/u/%d?filter=1&page=%d'%(user_id, page)\n",
    "            lxml = requests.get(url, cookies = cookie).content\n",
    "            \n",
    "            selector = etree.HTML(lxml)\n",
    "            content = selector.xpath('//span[@class = \"ctt\"]')\n",
    "            for each in content:\n",
    "                text = each.xpath('string(.)')\n",
    "                if word_count >= 3:\n",
    "                    text = \"%d: \"%(word_count - 2) + text + \"\\n\"\n",
    "                else:\n",
    "                    text = text + \"\\n\\n\"\n",
    "                result = result  + text\n",
    "                word_count += 1\n",
    "            print(page, 'word ok')\n",
    "            sys.stdout.flush()\n",
    "            soup = BeautifulSoup(lxml, \"lxml\")\n",
    "            urllist = soup.find_all('a', href=re.compile(r'^http://weibo.cn/mblog/oripic', re.I))\n",
    "            urllist1 = soup.find_all('a', href=re.compile(r'^http://weibo.cn/mblog/picAll', re.I))\n",
    "            for imgurl in urllist:\n",
    "                imgurl['href'] = re.sub(r\"amp;\", '', imgurl['href'])\n",
    "                urllist_set.add(requests.get(imgurl['href'], cookies = cookie).url)\n",
    "                image_count += 1\n",
    "            for imgurl_all in urllist1:\n",
    "                html_content = requests.get(imgurl_all['href'], cookies = cookie).content\n",
    "                soup = BeautifulSoup(html_content, \"lxml\")\n",
    "                urllist2 = soup.find_all('a', href = re.compile(r'^/mblog/oripic', re.I))\n",
    "                for imgurl in urllist2:\n",
    "                    imgurl['href'] = 'http://weibo.cn' + re.sub(r\"amp;\", '', imgurl['href'])\n",
    "                    urllist_set.add(requests.get(imgurl['href'], cookies = cookie).url)\n",
    "                    image_count += 1\n",
    "                image_count -= 1\n",
    "            print(page, 'picurl ok')\n",
    "        except:\n",
    "            print(page, 'error')\n",
    "        print(page, 'sleep')\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(60)\n",
    "    print(u'正在进行第', step + 1, u'次停顿，防止访问次数过多')\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文字微博爬取完畢\n",
      "圖片鏈接爬取完畢\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    fo = open(os.getcwd() + \"/%d\"%user_id, \"wb\")\n",
    "    fo.write(result.encode())\n",
    "    word_path = os.getcwd() + '/%d'%user_id \n",
    "    print(u'文字微博爬取完畢')\n",
    "    link = \"\"\n",
    "    fo2 = open(os.getcwd() + \"/%s_image\"%user_id, \"wb\")\n",
    "    for eachlink in urllist_set:\n",
    "        link = link + eachlink + \"\\n\"\n",
    "        fo2.write(link.encode())\n",
    "    print(u'圖片鏈接爬取完畢')\n",
    "except:\n",
    "    print(u'存放數據地址有誤')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://wx4.sinaimg.cn/large/73c7ca2fly1ff2fdd0b31j217e17e10i.jpg\n",
      "http://wx2.sinaimg.cn/large/73c7ca2fly1fibg6vsuo7j20ku0ku0vo.jpg\n",
      "http://wx2.sinaimg.cn/large/73c7ca2fly1ff0hol97slj217e17etig.jpg\n",
      "http://wx2.sinaimg.cn/large/73c7ca2fly1fmqjlt7xwjj20ku0kutmf.jpg\n",
      "http://wx3.sinaimg.cn/large/73c7ca2fly1ff15glzooxj20ku0kun45.jpg\n",
      "http://wx3.sinaimg.cn/large/73c7ca2fgy1fhrqxjqfr8j20ku0ku79d.jpg\n"
     ]
    }
   ],
   "source": [
    "for h in urllist_set:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35815"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下载第1张图片\n",
      "正在下载第2张图片\n",
      "正在下载第3张图片\n",
      "正在下载第4张图片\n",
      "正在下载第5张图片\n",
      "正在下载第6张图片\n",
      "正在下载第7张图片\n",
      "正在下载第8张图片\n",
      "正在下载第9张图片\n",
      "正在下载第10张图片\n",
      "正在下载第11张图片\n",
      "原创微博爬取完毕，共1713条，保存路径/Users/vincentyau/Documents/Python/LearingExercise/1942473263\n",
      "微博图片爬取完毕，共551张，保存路径/Users/vincentyau/Documents/Python/LearingExercise/weibo_image\n"
     ]
    }
   ],
   "source": [
    "if not urllist_set:\n",
    "    print('该用户原创微博中不存在图片')\n",
    "else:\n",
    "    image_path = os.getcwd() + '/weibo_image'\n",
    "    if os.path.exists(image_path) is False:\n",
    "        os.mkdir(image_path)\n",
    "    x = 1\n",
    "    for imgurl in urllist_set:\n",
    "        temp = image_path + '/%s.jpg' % x\n",
    "        print('正在下载第%s张图片'%x)\n",
    "        try:\n",
    "            r = requests.get(imgurl, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(temp, 'wb') as f:\n",
    "                    r.raw.decode_content = True\n",
    "                    shutil.copyfileobj(r.raw, f)\n",
    "        except:\n",
    "            print('该图片下载失败:%s'%imgurl)\n",
    "        x += 1\n",
    "print('原创微博爬取完毕，共%d条，保存路径%s'%(word_count - 3, word_path))\n",
    "print('微博图片爬取完毕，共%d张，保存路径%s'%(image_count - 1, image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urllist_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
